This blog post is my attempt to figure out what I think about AI, and why. It's an early draft: I am publishing it using the [permanent versions pattern](https://mathewlowry.medium.com/two-wiki-authors-and-a-blogger-walk-into-a-bar-7106c8376c6e) - where I publish a string of versions as I develop my thoughts (revision notes in footer) - because I'm hoping constructive reactions to early drafts will help me answer my questions and so finish the post.

Because I'm really torn about how I think about AI. I read a lot, and do more than just read: every time I read something valuable I take notes on it and publish them on my Hub, partly so others can benefit from the resulting library of [what I Like](https://myhub.ai/@mathewlowry/?quality=all&types=like&timeframe=anytime), but mostly so I fully integrate what I'm reading into [my work and writing](https://myhub.ai/@mathewlowry/?quality=all&types=do&types=think&timeframe=anytime). 

There are currently [324 resources tagged AI on my Hub](https://myhub.ai/@mathewlowry/?tags=ai&types=like&types=do&types=think&timeframe=anytime&quality=all). Many individual resources have really influenced me, from [ChatGPT as muse, not oracle](https://www.geoffreylitt.com/2023/02/26/llm-as-muse-not-oracle.html?utm_source=pocket_saves), [Ted Chiang's blurry jpeg](https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web?utm_source=pocket_saves), [Sara Walker's AI Is Life](https://www.noemamag.com/ai-is-life/) and [Emily Bender's stochastic parrots](https://nymag.com/intelligencer/article/ai-artificial-intelligence-chatbots-emily-m-bender.html?utm_source=pocket_saves)to  [What Do Large Language Models “Understand”?](https://towardsdatascience.com/what-do-large-language-models-understand-befdb4411b77). But I'm now at the point where I can identify some authors who are having a major influence.

The problem is this: they don't agree with each other. On the one hand there are "positives" (for lack of a better term) like Ethan Mollick, who publishes a stream of content about new developments in AI, and constantly reminds us that "the worse AI you'll ever use is the one you're using now"; and Kyle Shannon, who shows normal people like me using AI in really innovative, useful ways. 

Yet I'm also massively influenced by those who brilliantly set out why we are going in absolutely the wrong direction, like Cory Doctorow (10 [resources hubbed](https://myhub.ai/@mathewlowry/?tags=cory+doctorow&types=like&types=do&types=think&timeframe=anytime&quality=all)) and Douglas "Team Human" Rushkoff ([4 more](https://myhub.ai/@mathewlowry/?tags=douglas+rushkoff&types=like&types=do&types=think&timeframe=anytime&quality=all)).   

While the law of averages says that lying with your head in the freezer and your feet in the oven is  a good way of being comfortably warm, I find my position on AI, torn between the two, very uncomfortable. On the one hand I've been in the positive camp for over a decade - I was literally using NLP to autoclassify my clients' content in 2011-2012, was arguing for the integration of similar tools into EU Commission knowledge management in 2019, launched myhub.***ai*** the next year, and am currently working on integrating a pilot of tomorrow's "GPT@EC" into a major EC online platform.

So why was I so negative on AI when I published the results of my first experiments with integrating GPT and myhub.ai earlier this year? Why did I follow that with an apocalyptic Linkedin carousel about AI destroying society's ability to innovate by mid-century? Why did I not turn to ChatGPT to help me write this blog post?

It could be my own cognitive biases.  I find it very easy to write, for example, which probably explains why I am underwhelmed by ChatGPT: it spouts rubbish, cliche-ridden texts, and so doesn't offer me very much. On the other hand, even if it is rubbish today, there's a little voice in the back of my head telling me that it could be a future threat, because my writing ability is the one edge I was born with, and was literally the basis of my early career. AI could one day commoditise that ability, which is one reason I suspect why the writing and performance communities have been so vocal.

Then there's the bile I feel rise in my throat when I see people jump on a bandwagon. I've been optimistic and then disappointed about technology before. For example, the emergence of the blogosphere and then social media. All of these technologies offer so much to society, but there's a huge difference between what a technology can do and what actually happens with it. What actually happens is that people use the technology not for social good but for private gain. When that happens, they end up not fulfilling the promise of the technology for society and actually, in many ways, doing a lot of harm. I'm programmed in many ways to believe that's likely to happen with AI. Also, there's plenty of evidence that it will. 

---
Here's one example. I wrote a post pointing out that if many companies replace their junior people with AI, in 10 or 20 years, they're not going to have any middle to senior people left. And the problem there is that I don't believe AIs are actually that good at innovation. Humans are, but humans have to master their craft before they can build on it and innovate and stand on the shoulders of giants that came before them. But there won't be any people who will have mastered the craft in 20 years because they're not being hired now, their job is being done by AI. So we're hollowing out our expertise and our ability to innovate as a species. This is a risk that's pessimistic. Some people have reacted to that by pointing to posts by, for example, Ethan Mollick, pointing out that actually AIs can innovate. But those studies are more about ideation, which is part of innovation, but it's not a complete part. It's about generating lots of ideas, but not necessarily creating, taking a new idea and developing it into an innovation. That's a whole process and ideation is only the beginning. And even then, the study that people pointed to was a study where humans needed to evaluate the ideas in order to find the right one, because as I would point out, an AI can generate ideas, but it can't really spot the value in an idea. Not everybody agrees with me, not everybody is pessimistic. What's interesting is that the same poster on LinkedIn, Ethan Mollick, recently posted something else, which pointed out that they're not very good at innovating. So there seems to be a contradiction here, or contradictory ideas at least, and I want to explore that.

---






---

## Revision Notes

This is one of this wiki's pages managed with the **permanent versions pattern** described in  [Two wiki authors and a blogger walk into a bar…](https://mathewlowry.medium.com/two-wiki-authors-and-a-blogger-walk-into-a-bar-7106c8376c6e)  

- version control
    - this is version: 1
    - this is the current version: [[blowing hot and cold about AI]]
    - here is the previous version: n/a
